# Jailbreaking Deep Models: Adversarial Evaluation of ResNet-34 on ImageNet

## ğŸ“Œ Overview

This project investigates the adversarial robustness of a pretrained ResNet-34 model trained on the ImageNet-1K dataset. We implement and evaluate the impact of several adversarial attack techniques including:

- **Fast Gradient Sign Method (FGSM)**
- **Projected Gradient Descent (PGD)**
- **Patch-based Momentum Iterative FGSM (MI-FGSM)**

We also explore the **transferability** of adversarial examples to a different architecture, **DenseNet-121**, to analyze cross-model vulnerabilities. All experiments are built using PyTorch and executed under white-box assumptions.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ fgsm.py
â”‚   â”œâ”€â”€ pgd.py
â”‚   â””â”€â”€ patch_attack.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resnet34.py
â”‚   â””â”€â”€ densenet121.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ imagenet_subset/
â”‚   â””â”€â”€ labels_list.json
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluation_pipeline.ipynb
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ fgsm/
â”‚   â”œâ”€â”€ pgd/
â”‚   â””â”€â”€ patch/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§ª Attacks Implemented

### 1. FGSM (Fast Gradient Sign Method)
- Single-step, fast and efficient
- Perturbation budget: \( \epsilon = 0.02 \)

### 2. PGD (Projected Gradient Descent)
- Iterative extension of FGSM
- Perturbation budget: \( \epsilon = 0.02 \), step size \( \alpha = 0.005 \), 10 iterations

### 3. Patch-based MI-FGSM
- Perturbation confined to a 32Ã—32 region
- Budget: \( \epsilon \in [0.3, 0.5] \)
- Variants include random, fixed, and saliency-guided placements

## ğŸ“Š Results Summary

| **Model**       | **Attack** | **Top-1 Accuracy** | **Top-5 Accuracy** |
|----------------|------------|--------------------|--------------------|
| ResNet-34      | Baseline   | 76.00%             | 94.20%             |
|                | FGSM       | 6.40%              | 33.00%             |
|                | MI-FGSM    | 0.00%              | 12.00%             |
|                | Patch      | 9.00%              | 42.60%             |
| DenseNet-121   | Baseline   | 74.80%             | 93.60%             |
|                | FGSM       | 51.00%             | 79.00%             |
|                | MI-FGSM    | 46.40%             | 79.80%             |
|                | Patch      | 60.20%             | 87.80%             |

## ğŸš€ Running the Code

### Environment Setup

```bash
conda create -n adv-env python=3.7
conda activate adv-env
pip install -r requirements.txt
```

### Example Commands

```bash
# FGSM Attack
python attacks/fgsm.py --epsilon 0.02

# PGD Attack
python attacks/pgd.py --epsilon 0.02 --alpha 0.005 --steps 10

# Patch-based Attack
python attacks/patch_attack.py --epsilon 0.3 --patch_size 32 --mode random
```

## ğŸ”¬ Key Contributions

- Implemented reproducible adversarial attack pipeline on ImageNet subset.
- Quantified vulnerability of ResNet-34 and DenseNet-121 under white-box settings.
- Demonstrated high transferability of patch-based attacks.
- Provided visualizations and saved adversarial datasets for benchmarking.

## ğŸ“· Visual Examples

![FGSM Attack](results/fgsm/sample1.png)
![Patch Attack](results/patch/sample2.png)

## ğŸ“š References

- Goodfellow et al., *Explaining and Harnessing Adversarial Examples*, ICLR 2015
- Madry et al., *Towards Deep Learning Models Resistant to Adversarial Attacks*, ICLR 2018
- Dong et al., *Boosting Adversarial Attacks with Momentum*, CVPR 2018

## ğŸ‘¥ Authors

- Nikhil Bhise (nb4053@nyu.edu)  
- Vishwas Karale (vbk2750@nyu.edu)  
- Hemanth Sree Meka (hm3324@nyu.edu)
